#This code is a simple implementation of Deep Federated Learning Using MNIST Dataset

import torch
from torchvision import datasets, transforms
from torch import nn, optim
import torch.nn.functional as F
from torch.utils.data import DataLoader, SubsetRandomSampler
import numpy as np

# Number of clients and epochs for federated learning
num_clients = 2  # Set the number of clients participating in federated learning
num_epochs = 10  # Set the number of epochs for training

# Load MNIST dataset for training and testing
dataset_train = datasets.MNIST('../data', train=True, download=True,
                               transform=transforms.Compose([  # Transform images to tensor and normalize
                                   transforms.ToTensor(),
                                   transforms.Normalize((0.1307,), (0.3081,))  # Normalize with given mean and std
                               ]))

# Define the split sizes for training and testing datasets
train_size = int(0.8 * len(dataset_train))  # 80% of the dataset will be used for training
test_size = len(dataset_train) - train_size  # Remaining 20% will be used for testing

# Split the dataset into training and testing subsets
train_dataset = torch.utils.data.Subset(dataset_train, range(train_size))  # Training subset
test_dataset = torch.utils.data.Subset(dataset_train, range(train_size, len(dataset_train)))  # Testing subset

# Data loaders for training and testing to handle batching
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)  # Data loader for training, shuffle the data
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)  # Data loader for testing, no shuffling

# Define the SimpleCNN model for training in federated learning
class SimpleCNN(nn.Module):
    def __init__(self):
        super(SimpleCNN, self).__init__()  # Initialize the CNN class
        # Convolutional layer 1 with 1 input channel (grayscale) and 10 output channels (filters)
        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)
        # Convolutional layer 2 with 10 input channels and 20 output channels
        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)
        self.conv2_drop = nn.Dropout2d()  # Dropout layer to prevent overfitting
        self.fc1 = nn.Linear(320, 50)  # Fully connected layer from 320 input to 50 output
        self.fc2 = nn.Linear(50, 10)  # Fully connected layer from 50 input to 10 output (for 10 classes)

    def forward(self, x):
        # Apply convolutional layers with ReLU activation and max pooling
        x = F.relu(F.max_pool2d(self.conv1(x), 2))
        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))
        x = x.view(-1, 320)  # Flatten the tensor into a 1D vector
        x = F.relu(self.fc1(x))  # Apply ReLU activation to the fully connected layer
        x = F.dropout(x, training=self.training)  # Apply dropout during training
        x = self.fc2(x)  # Output layer
        return F.log_softmax(x, dim=1)  # Log softmax activation for multi-class classification

# Initialize the global model
global_model = SimpleCNN()  # Create an instance of the SimpleCNN model to serve as the global model

# Function to average the weights from each client and update the global model
def average_weights(local_weights):
    average_weights = {}  # Initialize a dictionary to store averaged weights
    for key in local_weights[0]:  # Loop over each weight parameter in the first clientâ€™s model
        average_weights[key] = torch.zeros_like(local_weights[0][key])  # Initialize weights with zeros
    for weights in local_weights:  # Loop over the weights of each client
        for key in weights:  # Loop over each weight parameter of the client
            average_weights[key] += weights[key]  # Add the client's weights to the averaged weights
    for key in average_weights:  # Normalize the averaged weights
        average_weights[key] = average_weights[key] / len(local_weights)  # Divide by the number of clients
    return average_weights  # Return the averaged weights

# Federated learning training loop
for epoch in range(num_epochs):  # Loop over each epoch
    local_weights = []  # List to store local model weights from each client
    epoch_accuracy = []  # List to store accuracy for the current epoch

    for client in range(num_clients):  # Loop over each client
        local_model = SimpleCNN()  # Instantiate a new model for the client
        local_model.load_state_dict(global_model.state_dict())  # Load the global model weights into the local model
        local_optimizer = optim.SGD(local_model.parameters(), lr=0.01, momentum=0.5)  # Define the optimizer

        # Data loader for a specific client (using random sampling)
        client_indices = np.random.permutation(len(train_dataset))[:len(train_dataset)//num_clients]  # Randomly select indices for this client
        client_loader = DataLoader(train_dataset, batch_size=64, sampler=SubsetRandomSampler(client_indices))  # Create data loader for the client

        local_model.train()  # Set the local model to training mode
        client_correct = 0  # Counter for correct predictions
        client_samples = 0  # Counter for total samples
        client_loss = 0  # Counter for the total loss

        # Loop through the data batches for this client
        for data, target in client_loader:
            local_optimizer.zero_grad()  # Zero the gradients before backward pass
            output = local_model(data)  # Forward pass through the local model
            loss = F.nll_loss(output, target)  # Compute the negative log likelihood loss
            loss.backward()  # Backward pass to compute gradients
            local_optimizer.step()  # Update model weights

            client_loss += loss.item() * data.size(0)  # Accumulate loss for this batch
            pred = output.argmax(dim=1, keepdim=True)  # Get the predicted class
            client_correct += pred.eq(target.view_as(pred)).sum().item()  # Count correct predictions
            client_samples += data.size(0)  # Count the number of samples processed

        client_accuracy = 100 * client_correct / client_samples  # Calculate accuracy for the client
        epoch_accuracy.append(client_accuracy)  # Store the client accuracy for this epoch
        local_weights.append(local_model.state_dict())  # Store the model weights for this client

        # Print the client's performance for this epoch
        print(f'Epoch {epoch+1}, Client {client+1}: Accuracy = {client_accuracy:.2f}%, Loss = {client_loss / client_samples:.4f}')

    # Average the weights from all clients and update the global model
    global_dict = average_weights(local_weights)  # Get the averaged weights from all clients
    global_model.load_state_dict(global_dict)  # Update the global model with the averaged weights
